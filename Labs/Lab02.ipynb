{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contacts\n",
    "For any question on the lab lectures please contact me by email at leonardo.cianfanelli@polito.it\n",
    "\n",
    "### Credits\n",
    "This notebook was created by Laura Arditti and Leonardo Cianfanelli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture outline\n",
    "- Katz and Bonacich centralities\n",
    "- iterative methods to compute centralities\n",
    "- network flows\n",
    "- exercise on algebraic graph theory (if we have time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Katz and Bonacich centralities\n",
    "1. Katz centrality \n",
    "$ z =  \\frac{1-\\beta}{\\lambda_W} W' z + \\beta \\mu $\n",
    "with $\\mu = \\mathbf{1}$ and $\\beta \\in [0,1]$ (typically $\\beta = 0.15$)\n",
    "2. Bonachich centrality \n",
    "$ z = (1-\\beta)P' z + \\beta \\mu $\n",
    "with $\\mu = \\mathbf{1}$ and $\\beta \\in [0,1]$ (typically $\\beta = 0.15$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary on centralities\n",
    "**Degree centrality**: the centrality of a node is proportional to its (in)degree. \n",
    "$$\n",
    "z_i = D_{ii}\n",
    "$$\n",
    "\n",
    "**Eigenvector centrality**: it takes into account that connections to more central nodes are more important.\n",
    "\n",
    "$$\n",
    "z = \\frac{W'z}{\\lambda_W}\n",
    "$$\n",
    "\n",
    "**Invariant distribution centrality**: it generalizes the eigenvector centrality by taking into account that being connected to nodes that connect to many nodes is less important than being connected to nodes that connect with a few nodes.\n",
    "\n",
    "$$\n",
    "z = P'z\n",
    "$$\n",
    "\n",
    "**Katz centrality**: it generalizes the eigenvector centrality by assuming that nodes have also an intrinsic centrality. The centrality is the sum of the intrinsic centrality and the centrality given by the network.\n",
    "\n",
    "$$\n",
    "z =  \\frac{1-\\beta}{\\lambda_W} W' z + \\beta \\mu \n",
    "$$\n",
    "\n",
    "**Bonacich centrality (or Page-rank)**: it generalizes the invariant distribution centrality by assuming that nodes have also an intrinsic centrality. The centrality is the sum of the intrinsic centrality and the centrality given by the network.\n",
    "$$ \n",
    "x = (1-\\beta)P' x + \\beta \\mu \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to compute Katz and Bonacich centralities: **direct** and **iterative**.\n",
    "We start by computing those centralities by direct methods for the Zachary's karate club graph.\n",
    "\n",
    "## Direct method (for didactic purposes)\n",
    "Direct methods consist in inverting the equation above and computing directly the centrality. Notice that\n",
    "\n",
    "1. the Katz centrality \n",
    "$ z =  (\\mathbf{I}-\\frac{1-\\beta}{\\lambda_W} W')^{-1} \\beta \\mu $\n",
    "\n",
    "2. and Bonacich centrality \n",
    "$ z = (\\mathbf{I}-(1-\\beta)P')^{-1} \\beta \\mu $\n",
    "\n",
    "Note that the inversion can be done because the matrices $\\frac{1-\\beta}{\\lambda_W} W'$ and $(1-\\beta)P'$ have spectral radius less than 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "G = nx.karate_club_graph()\n",
    "\n",
    "# compute matrices of the graph\n",
    "W = nx.adjacency_matrix(G)\n",
    "W = W.toarray()\n",
    "degrees = np.sum(W,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Katz centrality: $ z =  (\\mathbf{I}-\\frac{1-\\beta}{\\lambda_W} W')^{-1} \\beta \\mu $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = G.number_of_nodes() \n",
    "beta = 0.15\n",
    "mu = np.ones((N,1))\n",
    "# note that the normalization of mu does not influence z, once that the centrality gets normalized\n",
    "\n",
    "# compute the largest eigenvalue of W\n",
    "w,v = np.linalg.eig(W)\n",
    "w = w.real\n",
    "\n",
    "lambda_max = max(w) \n",
    "zk = np.linalg.inv(np.diag(np.ones(N)) - W.T*(1-beta)/lambda_max) * beta @ mu\n",
    "# normalize the centrality\n",
    "zk = zk/sum(zk)\n",
    "\n",
    "# sometimes it proves convenient to collect the centralities in a dictionary \n",
    "# with nodes as keys and their centrality as values\n",
    "zip_iterator = zip(G.nodes(), zk)\n",
    "zk_dict = dict(zip_iterator)\n",
    "\n",
    "print(zk_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the centralities are in array form because they are the result of the inversion of a matrix, \n",
    "# sometimes it can be convienent to convert them to float\n",
    "\n",
    "zk_list = [];\n",
    "for i in zk_dict.items():\n",
    "    # for every item (node,centrality), append to the list the centrality value\n",
    "    zk_list.append(float(i[1]))\n",
    "\n",
    "print(zk_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dictionary with zk_list as values\n",
    "zip_iterator = zip(G.nodes(), zk_list)\n",
    "zk_dict = dict(zip_iterator)\n",
    "\n",
    "print(zk_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonacich centrality\n",
    "Page rank centrality is the Bonacich centrality with $\\mu=\\mathbf{1}$ and $\\beta=0.15$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zb = np.linalg.inv(np.diag(np.ones(N)) - P.T*(1-beta)) * beta @ mu\n",
    "zb = zb/sum(zb)\n",
    "\n",
    "# transform centralities to float\n",
    "val = [];\n",
    "for i in zb:\n",
    "    val.append(float(i))\n",
    "\n",
    "zb_list = val\n",
    "    \n",
    "# create a dictionary to collect the centralities, with nodes as keys and their centrality as values\n",
    "zip_iterator = zip(G.nodes(), zb_list)\n",
    "zb_dict = dict(zip_iterator)\n",
    "\n",
    "print(zb_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute centralities by networkX functions\n",
    "The function `algorithms.link_analysis.pagerank_alg.pagerank` computes the Page-rank centrality of a given network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zb2_dict = nx.algorithms.link_analysis.pagerank_alg.pagerank(G)\n",
    "\n",
    "# check if the centrality are normalized\n",
    "zb2 = np.array(list(zb2_dict.values()))\n",
    "check = sum(zb2)\n",
    "print(\"Normalization:\", check, \"\\n\")\n",
    "\n",
    "# transform values to float\n",
    "zb2_list = [];\n",
    "for i in zb2:\n",
    "    zb2_list.append(float(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the page-rank centrality computed by the inversion formula with the one computed by NetworkX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for comparison, it is convenient to use centralities as arrays\n",
    "# before comparing we ensure that the shape of the arrays is the same\n",
    "\n",
    "print(\"Shape of zb:\", zb.shape)\n",
    "print(\"Shape of zb2\", zb2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since it is not, we reshape zb\n",
    "zb = zb.reshape(N)\n",
    "\n",
    "# now we can compute the distance\n",
    "print(\"Distance between zb and zb2:\", np.linalg.norm(zb-zb2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the distance between Bonacich centrality computed via direct method and networkX function is negligible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative methods\n",
    "The smartest way to compute Bonacich centrality (or Katz centrality) is to exploit iterative methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the dynamics\n",
    "$$\n",
    "\\begin{cases} \n",
    "z(t+1) = (1-\\beta)P'z(t) + \\beta \\mu  \\\\\n",
    "z(0) = z_0.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The transient of the dynamics is\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "z(1) = (1-\\beta)P'z(0) + \\beta \\mu\\\\\n",
    "z(2) = (1-\\beta)^2 (P')^2 z(0) + (1-\\beta)P' \\beta \\mu + \\beta \\mu\\\\\n",
    "\\vdots\\\\\n",
    "z(t) = (1-\\beta)^t (P')^t z(0) + \\sum_{i=0}^{t-1} (1-\\beta)^i (P')^i \\beta \\mu\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The first term vanishes as $t \\to +\\infty$ because $(1-\\beta)P'$ is sub-stochastic, while the second is a geometric sum. The dynamics thus converges to the limit\n",
    "\n",
    "$$\n",
    "\\lim_{t \\to +\\infty} z(t) = (\\mathbf{I}-(1-\\beta) P')^{-1} \\beta \\mu,\n",
    "$$\n",
    "\n",
    "which is the Bonacich centrality of the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**: you should never use direct ways to compute centralities if iterative algorithms are available. The iterative method is more efficient than the direct one as the order of the graph grows, since it does not involve the inversion of a matrix $N \\times N$.\n",
    "\n",
    "Note that the convergence of $z(t)$ to the Bonacich centrality holds for every initial condition $z(0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arbitrary initial condition: 1/N-uniform vector of size N\n",
    "z_0 = np.ones((N,1))/N\n",
    "# set a tolerance to assess convergence to the limit\n",
    "tol = 1e-5\n",
    "# evolve the dynamics\n",
    "z_old = z_0\n",
    "print()\n",
    "while True:\n",
    "    z_new = P.T @ z_old * (1-beta) + beta * mu\n",
    "    if np.linalg.norm(z_new-z_old) < tol:\n",
    "        break\n",
    "    z_old=z_new\n",
    "\n",
    "zb_approx = z_new\n",
    "\n",
    "# normalize the centrality\n",
    "zb_approx = zb_approx / sum(zb_approx)\n",
    "\n",
    "print(\"Approximation of zb_approx: \\n\", zb_approx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now compute the distance between zb and zb_approx\n",
    "zb_approx = zb_approx.reshape(N)\n",
    "print(\"Distance between zb and zb_approx:\", np.linalg.norm(zb-zb_approx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Compute the Katz centrality of the Zachary's karate club graph by iterative methods, and compare the results with direct methods.\n",
    "\n",
    "**Hint**: use the definition of Katz centrality and same techniques as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing sensitivity of measures\n",
    "In this section we will check the dependence of centrality measures with respect to their paramenters and the sensitivity of the iterative algorithms to compute such measure with respect to the number of iterations.\n",
    "\n",
    "## The effect of parameters\n",
    "In our first experiment we analyze the dependence of Page Rank centrality on the parameter $\\alpha=1-\\beta$. We set distinct values for $\\alpha$ while we fix the number of iterations, and run Page Rank. Then we plot the resulting Page Rank values with respect to $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(2, figsize=(16,7))\n",
    "ax1 = plt.subplot(121)\n",
    "ax2 = plt.subplot(122)\n",
    "\n",
    "pos = nx.spring_layout(G) \n",
    "nx.draw_networkx(G, pos, ax=ax1)\n",
    "\n",
    "# we consider values for alpha from 0.1 to 0.9 with step size 0.2\n",
    "alphas = np.arange(0.1, 0.9, 0.2)\n",
    "\n",
    "for alp in alphas:\n",
    "    # pagerank has parameters alpha and mu:\n",
    "    # note that alpha = 1-beta and weight parameters mu are set to 1 by default\n",
    "    pr = nx.pagerank(G, alpha=alp) \n",
    "    prval = list(pr.values())\n",
    "    ax2.plot(prval, color=np.random.rand(3), label='alpha {0:.2f}'.format(alp))\n",
    "    \n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "1. explain the previous result. \n",
    "\n",
    "**Hint:** keep in mind that the parameter alpha used by `nx.pagerank` corresponds to $1-\\beta$, and the centrality $z$ satisfies\n",
    "\n",
    "$$\n",
    "z = (1-\\beta)P'z + \\beta \\mu\n",
    "$$\n",
    "2. Repeat the analysis. This time keep $\\alpha$ fixed to 0.5 and select 3 different non-uniform vectors $\\mu$ as `personalization` parameter to `pagerank`. How do you interpret the result?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The effect of iteration number\n",
    "In this section we consider a bigger network and we analyse the speed of convergence of iterative algorithms for computing centrality measures. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the political blogs network (save it as a .gml file in the working directory of this notebook) and import it as a Graph object. We check the basic properties of G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_gml('polblogs.gml')\n",
    "print(\"Type of G:\", type(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since G is a multigraph, we define an equivalent graph to compute the centralities (the function 'pagerank' does not work with multigraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GG = nx.Graph()\n",
    "for n, nbrs in G.adjacency():\n",
    "    # edict is a dictionary of dictionaries; \n",
    "    # the keys of edict are parallel edges from n to nbr;\n",
    "    # the values of edict are dictionary,\n",
    "    # containing attribute values of the corresponding edge\n",
    "    for nbr, edict in nbrs.items(): \n",
    "        # each edge has weight=1, so total value is just  \n",
    "        # the number of parallel edges\n",
    "        total_value = len(edict) \n",
    "        GG.add_edge(n, nbr, weight = total_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph is very large, thus we cannot plot it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now test the convergence speed of `nx.pagerank` algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(16,7))\n",
    "\n",
    "# set 3 iteration numbers\n",
    "iters = [10,15,50]\n",
    "# define the position of the next plot in the subplot grid\n",
    "position = 1\n",
    "# create a list to collect the page rank values obtained in the three runs \n",
    "prvals = []\n",
    "\n",
    "for max_iter in iters:\n",
    "    # compute page rank\n",
    "    pr = nx.pagerank(GG, max_iter = max_iter) \n",
    "    # compute page rank values\n",
    "    prval = list(pr.values())\n",
    "    # append the result to the list\n",
    "    prvals.append(np.array(prval)) \n",
    "    # create a new sublot in the grid\n",
    "    ax = fig.add_subplot(2,2,position)\n",
    "    # plot the PR values\n",
    "    ax.plot(prval, color=np.random.rand(3), label='{0:d} iterations'.format(max_iter))\n",
    "    position+=1\n",
    "\n",
    "# add a legend which contains all label\n",
    "# informations specified in previous plot calls\n",
    "fig.legend()  \n",
    "# we assume the values obtained with nx.pagerank()\n",
    "# with no iterations constraints as a benchmark\n",
    "benchmark = np.array(list(nx.pagerank(GG).values())) \n",
    "# we compute errors as norm of the differences wrt the benchmark\n",
    "errors = [np.linalg.norm(prval-benchmark) for prval in prvals]\n",
    "print(\"Errors:\", errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nx.pagerank` algorithm converges very fast, in 10 iterations! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Check if our iterative algorithm for computing Bonacich centrality is as good as this by performing a similar analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An interpretation of Katz (and Bonacich) centrality\n",
    "Bonacich centrality (as well as Katz centrality) can be interpreted in terms of paths on the graphs.\n",
    "We show this by an example.\n",
    "\n",
    "We shall make use of an undirected graph and Katz centrality to simplify the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.lollipop_graph(6,3)\n",
    "G.add_edges_from([(9,8),(10,8),(11,8),(12,8),(13,8)])\n",
    "\n",
    "pos = nx.spring_layout(G) \n",
    "nx.draw(G, pos, with_labels = True)\n",
    "\n",
    "N = len(G)\n",
    "\n",
    "# compute matrices of the graph\n",
    "W = nx.adjacency_matrix(G)\n",
    "W = W.toarray()\n",
    "degrees = np.sum(W,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ W\n",
    "\n",
    "# compute the largest eigenvalue of W\n",
    "w,v = np.linalg.eig(W)\n",
    "w = w.real\n",
    "\n",
    "lambda_max = max(w) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a uniform centrality distribution, and consider uniform $\\mu$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.ones((N,1))\n",
    "beta = 0.15\n",
    "# initial centrality distribution\n",
    "z = np.ones((N,1))/N\n",
    "z_reshape = z.reshape(N)\n",
    "\n",
    "nodesize=z_reshape*7000\n",
    "\n",
    "print(\"Centralities at iteration 0:\", z_reshape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that normalizing $\\mu$ does not modify the centrality, since in\n",
    "$$ \n",
    "z =  (\\mathbf{I}-\\frac{1-\\beta}{\\lambda_W} W')^{-1} \\beta \\mu \n",
    "$$\n",
    "$\\mu$ affects the normalization of $z$ only (same argument holds for the Bonacich centrality). \n",
    "\n",
    "However, the normalization of $\\mu$ affects the transient of the iteration to compute $z$. In particular, if one considers the Bonacich centrality, using $\\mu$ such that $\\mathbf{1}' \\mu = 1$ is preferable, since it guarantees that, if $\\mathbf{1}' z(0)=1$, then $\\mathbf{1}' z(t)=1$ for every $t$. Indeed,\n",
    "\n",
    "$$\n",
    "\\mathbf{1}' z(t+1) = (1-\\beta) \\mathbf{1}' P' z(t) + \\beta \\mathbf{1}'\\mu = (1-\\beta) \\mathbf{1}' z(t) + \\beta = (1-\\beta) + \\beta = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to Katz centrality\n",
    "\n",
    "After 1 iteration,\n",
    "$$\n",
    "z(1) = \\frac{(1-\\beta)}{\\lambda_W}W'z(0) + \\beta \\mu,\n",
    "$$\n",
    "\n",
    "which means that the centrality of a node is the a combination of its intrinsic centrality, and the centrality of the neighbors. Since the intrinsic centrality is the same for every node, if we start with a uniform $z(0)$, $z(1)$ depends only on the degree of the node.\n",
    "\n",
    "Similar observation can be made for the Bonacich centrality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after 1 iteration\n",
    "z = W.T @ z * (1-beta)/lambda_max + beta * mu\n",
    "\n",
    "z_reshape = z.reshape(N)\n",
    "\n",
    "nodesize=z_reshape*7000\n",
    "\n",
    "# plot centrality at iteration 0\n",
    "plt.figure(1, figsize=(10,7))\n",
    "# we draw the graph with same node position \"pos\" defined above\n",
    "nx.draw(G,pos,\n",
    "         with_labels=True,\n",
    "         nodelist=list(G.nodes()), \n",
    "         # node size is proportional to centrality value\n",
    "         node_size = nodesize, \n",
    "         # node's color reflects centrality values (higher dc = darker color)\n",
    "         node_color=z_reshape,\n",
    "         font_size=8,\n",
    "         # node's colors are on the red scale\n",
    "         cmap=plt.cm.Reds) \n",
    "\n",
    "print(\"Centralities at iteration 1:\", z_reshape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the number of iteration grows, $z(n)$ takes into account also nodes at greater distance.\n",
    "\n",
    "At the equilibrium, the centrality $z^*$ can be interpreted in terms of paths as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "z^*&=\\lim_{n \\to +\\infty}z(n)=\\sum_{n = 0}^{\\infty} \\left(\\frac{(1-\\beta)}{\\lambda_W}\\right)^n (W')^n \\beta \\mu \\\\\n",
    "   &= \\beta \\mu + \\frac{(1-\\beta)}{\\lambda_W} (W') \\beta \\mu + \\left(\\frac{(1-\\beta)}{\\lambda_W}\\right)^2 (W')^2 \\beta \\mu + \\cdots\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "If one considers node $i$\n",
    "\n",
    "$$\n",
    "z^*_i = \\beta \\mu_i + \\frac{(1-\\beta)}{\\lambda_W}\\beta \\sum_{j} (W')_{ij} \\mu_j + \\left(\\frac{(1-\\beta)}{\\lambda_W}\\right)^2 \\beta \\sum_{j} ((W')^2)_{ij} \\mu_j + \\cdots\n",
    "$$\n",
    "\n",
    "**Interpretation**. Since $((W')^n)_{ij}$ is the number of paths of length 'n' from 'j' to 'i', the centrality of node 'i' is the sum of:\n",
    "- its intrinsic centrality, plus \n",
    "- the intrinsic centrality of its neighbors, plus \n",
    "- the intrinsic centrality of the nodes connected by paths of length 2, and so on... \n",
    "\n",
    "Longer paths have a decreasing weight due to the term $(1-\\beta)^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: which node do you expect to have a higher Katz centrality? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# direct method for Katz (do not use direct method in homeworks!!!)\n",
    "\n",
    "zk = np.linalg.inv(np.diag(np.ones(N)) - W.T*(1-beta)/lambda_max) * beta @ mu\n",
    "# normalize the centrality\n",
    "zk = zk/sum(zk)\n",
    "zk = zk.reshape(N)\n",
    "\n",
    "print(zk)\n",
    "\n",
    "plt.figure(1, figsize=(10,7))\n",
    "# we draw the graph with same node position \"pos\" defined above\n",
    "nx.draw(G,pos,\n",
    "         with_labels=True,\n",
    "         nodelist=list(G.nodes()), \n",
    "         # node size is proportional to centrality value\n",
    "         node_size = zk*7000, \n",
    "         # node's color reflects centrality values (higher dc = darker color)\n",
    "         node_color=zk,\n",
    "         font_size=8,\n",
    "         # node's colors are on the red scale\n",
    "         cmap=plt.cm.Reds) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: how do you expect to be modified the centralities when using Bonacich instead of Katz? Focus on node 8.\n",
    "\n",
    "**Hint**: recall the definition of the two centralities, i.e.,\n",
    "\n",
    "- Katz: $z =  \\frac{1-\\beta}{\\lambda_W} W' z + \\beta \\mu$\n",
    "- Bonacich: $x = (1-\\beta)P' x + \\beta \\mu$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zb_dict = nx.algorithms.link_analysis.pagerank_alg.pagerank(G)\n",
    "\n",
    "# check if the centrality are normalized\n",
    "zb = np.array(list(zb_dict.values()))\n",
    "\n",
    "print(zk)\n",
    "\n",
    "plt.figure(1, figsize=(10,7))\n",
    "# we draw the graph with same node position \"pos\" defined above\n",
    "nx.draw(G,pos,\n",
    "         with_labels=True,\n",
    "         nodelist=list(G.nodes()), \n",
    "         # node size is proportional to centrality value\n",
    "         node_size = zb*7000, \n",
    "         # node's color reflects centrality values (higher dc = darker color)\n",
    "         node_color=zb,\n",
    "         font_size=8,\n",
    "         # node's colors are on the red scale\n",
    "         cmap=plt.cm.Reds) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Compute all the centralities for the this graph, plot them, and comment the results.\n",
    "\n",
    "**Hint**: use the code introduced in the lectures to compute the degree, eigenvector, Katz and Bonacich centralities. For the invariant distribution centrality use the function `np.linalg.eig()` to find the invariant distribution of $P$. Use the code introduced in the last lecture to plot the centralities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flows on graphs\n",
    "In this lab we discuss how maximum admissible flows on a capacitated network are related with cuts in the network by the Max Flow - Min Cut theorem. \n",
    "\n",
    "Let us start with the definition of flows from a source 's' to a destination 't' (called terminal nodes).\n",
    "\n",
    "**Definition**: a s-t flow is a distribution such that:\n",
    "- for every non-terminal node the incoming flow equals the outcoming flow (mass conservation);\n",
    "- the outcoming flow from 's' equals the incoming flow to 't' (this quantity is called 'throughput').\n",
    "\n",
    "### Example\n",
    "The blue edge labels indicate the flow along the edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "G.add_edges_from([(0,1),(0,2),(1,2),(2,3),(1,3)])\n",
    "\n",
    "pos = nx.spring_layout(G)\n",
    "\n",
    "# script to draw edge labels\n",
    "nx.draw_networkx_edge_labels(G,pos,edge_labels={(0,1):'4',\n",
    "(0,2):'1',(1,2):'2',(1,3):'2',\n",
    "(2,3):'3'},font_color='blue')\n",
    "\n",
    "nx.draw(G, pos, with_labels = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the flow distribution labelled in blue is a flow from 0 to 3. Indeed,\n",
    "- for the non-terminal nodes 1 and 2, the incoming flow equals the outcoming flow;\n",
    "- the total flow outcoming from 0 equals the total flow incoming to 3 (the throughput is 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Flow - Min Cut Theorem and the Ford Fulkerson algorithm\n",
    "It is given a graph $G = (V, E)$ which represents a flow network and two vertices source ‘s’ and sink ‘t’ in it. Every edge $(u,v)$ has a capacity $c(u,v)$. We want to find the maximum possible flow from s to t with the following constraint:\n",
    "\n",
    "1. Flow on an edge doesn’t exceed the given capacity of the edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_networkx_edge_labels(G,pos,edge_labels={(0,1):'2',\n",
    "(0,2):'1',(1,2):'4',(1,3):'3',\n",
    "(2,3):'2'},font_color='red')\n",
    "\n",
    "nx.draw(G, pos, with_labels = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: consider the graph above with terminal nodes 0 and 3, where the red labels denote the capacity of the edge, i.e., the maximal flow that can be sent along the edge. What is the maximal flow that can be send from 0 to 3?\n",
    "\n",
    "**Question**: give an intuitive answer on why a flow of throughput 4 cannot be sent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition (cut of a network)**: a s-t cut of the network $G=(N,E)$ is a partition of the nodes $\\{U,U^C\\}$, such that $s \\in U$ and $t \\in U_C$.\n",
    "\n",
    "We shall see that the notion of cut is strongly related to the maximal flow that can be sent in the network. The cuts of the network above are:\n",
    "- $U=\\{0,1,2\\},U^C=\\{3\\}$\n",
    "- $U=\\{0,1\\},U^C=\\{2,3\\}$\n",
    "- $U=\\{0,2\\},U^C=\\{1,3\\}$\n",
    "- $U=\\{0\\},U^C=\\{1,2,3\\}$\n",
    "\n",
    "**Definition (cut capacity)**: the capacity of a cut $\\{U,U^C\\}$ is\n",
    "$$\n",
    "C_{U} = \\sum_{i \\in U}\\sum_{j \\in U^C} c(i,j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Max flow min cut Theorem**: the maximal flow that can send from 's' to 't' equals the minimal cut capacity among the s-t cuts of the network. \n",
    "\n",
    "**Back to the question**:\n",
    "- $U=\\{0,1,2\\},U^C=\\{3\\}$ -> $C_U = 5$\n",
    "- $U=\\{0,1\\},U^C=\\{2,3\\}$ -> $C_U = 8$\n",
    "- $U=\\{0,2\\},U^C=\\{1,3\\}$ -> $C_U = 4$\n",
    "- $U=\\{0\\},U^C=\\{1,2,3\\}$ -> $C_U = 3$\n",
    "\n",
    "**Answer**: the 0-3 min-cut has capacity 3, thus the maximal flow from 0 to 3 is 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NetworkX has many functions useful for flow applications, e.g., `networkx.algorithms.flow.maximum_flow` and `networkx.algorithms.flow.minimum_cut`, which compute the maximum throughput and the value and the node partition of a minimum cut, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find the maximal flow, the edges have to be labelled with a 'capacity' label\n",
    "# we then modify the graph in such a way to include this information \n",
    "# (check the `networkx.algorithms.flow.maximum_flow` documentation for more info)\n",
    "\n",
    "G[0][1]['capacity'] = 2\n",
    "G[0][2]['capacity'] = 1\n",
    "G[1][2]['capacity'] = 4\n",
    "G[1][3]['capacity'] = 3\n",
    "G[2][3]['capacity'] = 2\n",
    "\n",
    "nx.algorithms.flow.maximum_flow(G,0,3)\n",
    "\n",
    "# maximum_flow returns the maximal throughput, plus a dictionary containing the value of the flow that went through each edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.algorithms.flow.minimum_cut(G,0,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intervention in capacitated networks: two dual problems\n",
    "\n",
    "### Adversarial intervention\n",
    "**Question**: suppose you are an adversarial agent that aims at minimizing the flow that can be send from 0 to 3 by removing capacity subject to a budget constraint (or even disconnect the network). \n",
    "- Where do you remove the capacity? \n",
    "- What is the minimal capacity that you need to remove in order to disconnect the network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** \n",
    "- Since the bottleneck of the flow is the min-cut of the network, the adversary should reduce the capacity of such a  cut. Thus, in this case, the adversary should reduce capacity of edges between the sets $\\{0\\}$ and the set $\\{1,2,3\\}$, which are $(0,1)$ and $(0,2)$. \n",
    "- The minimal capacity that needs to be removed equals the capacity of the min-cut of the network, which is 3.\n",
    "\n",
    "While for this network the answer could be obvious, when the network is large the algorithms `networkx.algorithms.flow.maximum_flow` and `networkx.algorithms.flow.minimum_cut` are very useful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_networkx_edge_labels(G,pos,edge_labels={(0,1):'2',\n",
    "(0,2):'1',(1,2):'4',(1,3):'3',\n",
    "(2,3):'2'},font_color='red')\n",
    "\n",
    "nx.draw(G, pos, with_labels = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cuts**\n",
    "- $U=\\{0,1,2\\},U^C=\\{3\\}$ -> $C_U = 5$\n",
    "- $U=\\{0,1\\},U^C=\\{2,3\\}$ -> $C_U = 8$\n",
    "- $U=\\{0,2\\},U^C=\\{1,3\\}$ -> $C_U = 4$\n",
    "- $U=\\{0\\},U^C=\\{1,2,3\\}$ -> $C_U = 3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planner intervention\n",
    "**Question**: Suppose you are a planner that aims at maximing the flow that can be send from 0 to 3. \n",
    "- Where do you allocate the capacity?\n",
    "\n",
    "**Answer**: \n",
    "- you should allocate the capacity in such a way that the capacity of the mincut is maximized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.remove_edge(0,2)\n",
    "\n",
    "nx.draw_networkx_edge_labels(G,pos,edge_labels={(0,1):'2',(1,2):'4',(1,3):'1',\n",
    "(2,3):'3'},font_color='red')\n",
    "\n",
    "nx.draw(G, pos, with_labels = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem**: suppose you can allocate a capacity 4 (that can be distributed on several links). Where do you allocate it?\n",
    "\n",
    "**Solution**: first compute the capacity of the cuts:\n",
    "- $U=\\{0,1,2\\},U^C=\\{3\\}$ -> $C_U = 4$\n",
    "- $U=\\{0,1\\},U^C=\\{2,3\\}$ -> $C_U = 5$\n",
    "- $U=\\{0,2\\},U^C=\\{1,3\\}$ -> $C_U = 5$\n",
    "- $U=\\{0\\},U^C=\\{1,2,3\\}$ -> $C_U = 2$\n",
    "\n",
    "The min-cut is $\\{0\\},\\{1,2,3\\}$, thus the capacity should be allocated on the links between these set, i.e., on $(0,1)$.\n",
    "\n",
    "Is allocating capacity 4 on $(0,1)$ optimal? NO!\n",
    "\n",
    "In such a case, the cut capacities become:\n",
    "\n",
    "- $U=\\{0,1,2\\},U^C=\\{3\\}$ -> $C_U = 4$\n",
    "- $U=\\{0,1\\},U^C=\\{2,3\\}$ -> $C_U = 5$\n",
    "- $U=\\{0,2\\},U^C=\\{1,3\\}$ -> $C_U = 5$\n",
    "- $U=\\{0\\},U^C=\\{1,2,3\\}$ -> $C_U = 6$\n",
    "\n",
    "and the corresponding throughput is 4.\n",
    "\n",
    "Note instead that after allocating a capacity 2 on edge $(0,1)$, the cut capacities become\n",
    "\n",
    "- $U=\\{0,1,2\\},U^C=\\{3\\}$ -> $C_U = 4$\n",
    "- $U=\\{0,1\\},U^C=\\{2,3\\}$ -> $C_U = 5$\n",
    "- $U=\\{0,2\\},U^C=\\{1,3\\}$ -> $C_U = 5$\n",
    "- $U=\\{0\\},U^C=\\{1,2,3\\}$ -> $C_U = 4$,\n",
    "\n",
    "with still a capacity 2 to be allocated. This remaining capacity should be allocated on both the first and the last cut, so the final solution is to allocate capacity 3 on the edge $(0,1)$, and 1 on the edge $(1,3)$ or $(2,3)$, which leads to cuts\n",
    "\n",
    "- $U=\\{0,1,2\\},U^C=\\{3\\}$ -> $C_U = 5$\n",
    "- $U=\\{0,1\\},U^C=\\{2,3\\}$ -> $C_U = 5$\n",
    "- $U=\\{0,2\\},U^C=\\{1,3\\}$ -> $C_U = 5$\n",
    "- $U=\\{0\\},U^C=\\{1,2,3\\}$ -> $C_U = 5$,\n",
    "\n",
    "and maximal throughput 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical example 2\n",
    "In some cases, it is possible that the capacity of two min-cuts can be increased by improving only one edge.\n",
    "Consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.add_edge(0,2)\n",
    "G.remove_edge(1,3)\n",
    "\n",
    "nx.draw_networkx_edge_labels(G,pos,edge_labels={(0,1):'1',(0,2):'1',(1,2):'1',\n",
    "(2,3):'3'},font_color='red')\n",
    "\n",
    "nx.draw(G, pos, with_labels = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: what is the maximal throughput that can be send from 0 to 3?\n",
    "\n",
    "The answer could be given by using NetworkX functions, but we do not use it now.\n",
    "\n",
    "**Answer**: we start by computing the cut capacities:\n",
    "\n",
    "- $U=\\{0,1,2\\},U^C=\\{3\\}$ -> $C_U = 3$\n",
    "- $U=\\{0,1\\},U^C=\\{2,3\\}$ -> $C_U = 2$\n",
    "- $U=\\{0,2\\},U^C=\\{1,3\\}$ -> $C_U = 4$\n",
    "- $U=\\{0\\},U^C=\\{1,2,3\\}$ -> $C_U = 2$.\n",
    "\n",
    "The maximal throughput is thus 2.\n",
    "\n",
    "**Question 2**: suppose you can allocate a capacity 1 on the network. Where do you allocate it?\n",
    "\n",
    "**Answer**: there are two mincuts, whose capacity is 2. \n",
    "- For the mincut $U=\\{0,1\\},U^C=\\{2,3\\}$,\n",
    "$$\n",
    "C_U = c(0,2) + c(1,2)\n",
    "$$ \n",
    "- For the mincut $U=\\{0\\},U^C=\\{1,2,3\\}$,\n",
    "$$\n",
    "C_U = c(0,1) + c(0,2)\n",
    "$$\n",
    "\n",
    "Note that both the capacities can be improved by adding capacity 1 to the edge $(0,2)$. By doing this, the capacity become\n",
    "\n",
    "- $U=\\{0,1,2\\},U^C=\\{3\\}$ -> $C_U = 3$\n",
    "- $U=\\{0,1\\},U^C=\\{2,3\\}$ -> $C_U = 3$\n",
    "- $U=\\{0,2\\},U^C=\\{1,3\\}$ -> $C_U = 4$\n",
    "- $U=\\{0\\},U^C=\\{1,2,3\\}$ -> $C_U = 3$,\n",
    "\n",
    "and the maximal throughput become 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, these type of considerations can be implemented algorithmically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algebraic graph theory\n",
    "Let us solve this exercise left to do in the last lecture\n",
    "\n",
    "### Exercise\n",
    "We are given the following graph G. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(range(0,10))\n",
    "nx.add_cycle(G,[0,1,2])\n",
    "nx.add_cycle(G,[3,4,5])\n",
    "nx.add_cycle(G,[7,8,9])\n",
    "G.add_edges_from([(3,2), (2,7), (4,6), (6,6)])\n",
    "\n",
    "# define pos according to spring layout\n",
    "# to fix nodes' positions in all graph drawings.\n",
    "# spring_layout positions nodes using Fruchterman-Reingold force-directed algorithm.\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G,pos, with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the computation of the extremal eigenvectors of G.\n",
    "\n",
    "1. Find the attractive components of G\n",
    "2. For each attractive component, construct the corresponding induced subgraph\n",
    "3. Compute the P matrix of each induced subgraph and its invariant measure\n",
    "4. Map the obtained measures back to the original graph G (by adding zeros in the appropriate positions)\n",
    "\n",
    "**Hint**: use the methods introduced in the previous notebook, the code of the last lecture, and the following theorem\n",
    "\n",
    "**Theorem**: the multeplicity of the eigenvalue 1 equals the number of attractive components (or trapping sets, as defined in the lecture notes) of the graph. Moreover, for every attractive component, there exists a dominant eigenvector (called extremal) whose support is exactly the node set of the attractive component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point 1: find the attractive components by using NetworkX functions\n",
    "attr_components = tuple(nx.algorithms.components.attracting_components(G))\n",
    "\n",
    "print(attr_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in attr_components:\n",
    "    # Point 2: construct the induced subgraph with nodes from the attractive component c\n",
    "    sG = G.subgraph(c)\n",
    "    # Point 3: compute the P matrix of each induced subgraph\n",
    "    # construct the matrix P on the subgraph\n",
    "    W = nx.adjacency_matrix(sG)\n",
    "    W = W.toarray()\n",
    "    degrees = np.sum(W,axis=1)\n",
    "    D = np.diag(degrees)\n",
    "    P = np.linalg.inv(D) @ W\n",
    "    # find the extremal dominant eigenvector corresponding to component c\n",
    "    w,v = np.linalg.eig(P.T)\n",
    "    for index in [i for i in range(len(sG)) if np.isclose(w[i],1)]: \n",
    "        pi = v[:,index].real  # -> eigenvectors are complex but pi is real, so we convert it to real\n",
    "        pi = pi/np.sum(pi)\n",
    "    # map pi back in the original node space\n",
    "    pi_G = np.zeros(len(G))\n",
    "    for i in range(len(sG)):\n",
    "        pi_G[list(sG.nodes)[i]] = pi[i]\n",
    "    print(\"pi:\", pi_G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the matrix $P$ of the induced subgraph with node set $c$ is equivalent to the restriction of the original $P$ on the node set $c$ (except for a permutation of the nodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute P on the induced subgraphs and print it\n",
    "for c in attr_components:\n",
    "    sG = G.subgraph(c)\n",
    "    # construct the matrix P on the subgraph\n",
    "    W = nx.adjacency_matrix(sG)\n",
    "    W = W.toarray()\n",
    "    degrees = np.sum(W,axis=1)\n",
    "    D = np.diag(degrees)\n",
    "    P = np.linalg.inv(D) @ W\n",
    "    print(\"P on the subgraph c: \\n\", P, \"\\n\")\n",
    "    \n",
    "# compute the original P and print it\n",
    "W = nx.adjacency_matrix(G)\n",
    "W = W.toarray()\n",
    "degrees = np.sum(W,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ W\n",
    "print(\"The original P: \\n\", P, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is due to the fact that 'c' are attractive components, thus they do not have links that connect nodes in 'c' to nodes not belonging to 'c'.\n",
    "\n",
    "Thus, $P_c$ could be obtained without recomputing the matrices of the graph $G_c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise Katz centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = nx.adjacency_matrix(G)\n",
    "W = W.toarray()\n",
    "\n",
    "# compute the largest eigenvalue of W\n",
    "w,v = np.linalg.eig(W)\n",
    "w = w.real\n",
    "\n",
    "lambda_max = max(w) \n",
    "\n",
    "beta = 0.15\n",
    "mu = np.ones((N,1))\n",
    "\n",
    "z_0 = np.ones((N,1))/N\n",
    "# set a tolerance to assess convergence to the limit\n",
    "tol = 1e-5\n",
    "# evolve the dynamics\n",
    "z_old = z_0\n",
    "\n",
    "while True:\n",
    "    z_new = W.T @ z_old * (1-beta)/lambda_max + beta * mu\n",
    "    if np.linalg.norm(z_new-z_old) < tol:\n",
    "        break\n",
    "    z_old=z_new\n",
    "\n",
    "zk_approx = z_new\n",
    "\n",
    "# normalize the centrality\n",
    "zk_approx = zk_approx / sum(zk_approx)\n",
    "\n",
    "# print the shapes of the two centralities\n",
    "print(zk_approx.shape)\n",
    "print(zk.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since they are in same shape, the comparison can be done\n",
    "print(\"Distance:\", np.linalg.norm(zk_approx-zk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise speed of convergence Bonacich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(16,7))\n",
    "\n",
    "# set 3 iteration numbers\n",
    "iters = [10,15,50]\n",
    "# define the position of the next plot in the subplot grid\n",
    "position = 1\n",
    "# create a list to collect the page rank values obtained in the three runs \n",
    "prvals = []\n",
    "\n",
    "for max_iter in iters:\n",
    "    # compute page rank\n",
    "    z_0 = np.ones((N,1))/N\n",
    "    z_old = z_0\n",
    "    \n",
    "    while True:\n",
    "        z_new = P.T @ z_old * (1-beta) + beta * mu\n",
    "        if np.linalg.norm(z_new-z_old) < tol:\n",
    "            break\n",
    "        z_old=z_new\n",
    "\n",
    "    pr = z_new\n",
    "\n",
    "    # normalize the centrality\n",
    "    prval = pr / sum(pr)\n",
    "    \n",
    "    # append the result to the list\n",
    "    prvals.append(np.array(prval)) \n",
    "    # create a new sublot in the grid\n",
    "    ax = fig.add_subplot(2,2,position)\n",
    "    # plot the PR values\n",
    "    ax.plot(prval, color=np.random.rand(3), label='{0:d} iterations'.format(max_iter))\n",
    "    position+=1\n",
    "\n",
    "# add a legend which contains all label\n",
    "# informations specified in previous plot calls\n",
    "fig.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
